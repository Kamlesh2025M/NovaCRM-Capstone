"""
Compare Metrics Before/After M4

Compares metrics from two measurement runs to show improvement
"""

import json
import sys
from pathlib import Path
from typing import Dict, Any

def load_report(filepath: str) -> Dict[str, Any]:
    """Load metrics report from JSON file"""
    with open(filepath, 'r') as f:
        return json.load(f)

def compare_metrics(before: Dict[str, Any], after: Dict[str, Any]):
    """Compare two metric reports and show improvements"""
    
    print("\n" + "=" * 80)
    print(" " * 25 + "METRICS COMPARISON")
    print("=" * 80)
    
    print(f"\nBefore: {before.get('timestamp', 'Unknown')}")
    print(f"After:  {after.get('timestamp', 'Unknown')}")
    
    print("\n" + "-" * 80)
    print(f"{'Metric':<40} {'Before':<12} {'After':<12} {'Change':<12}")
    print("-" * 80)
    
    # Extract metrics
    before_metrics = before['metrics']
    after_metrics = after['metrics']
    
    comparisons = [
        ('Classification Accuracy', 
         before_metrics['classification_accuracy']['accuracy'],
         after_metrics['classification_accuracy']['accuracy']),
        
        ('Answer Grounding Rate',
         before_metrics['answer_grounding']['grounding_rate'],
         after_metrics['answer_grounding']['grounding_rate']),
        
        ('PII Protection Rate',
         before_metrics['pii_protection']['protection_rate'],
         after_metrics['pii_protection']['protection_rate']),
        
        ('Professional Tone Compliance',
         before_metrics['professional_tone']['compliance_rate'],
         after_metrics['professional_tone']['compliance_rate']),
        
        ('Error Handling Rate',
         before_metrics['error_handling']['handling_rate'],
         after_metrics['error_handling']['handling_rate']),
        
        ('Sensitive Content Escalation',
         before_metrics['sensitive_escalation']['escalation_rate'],
         after_metrics['sensitive_escalation']['escalation_rate']),
    ]
    
    for metric_name, before_val, after_val in comparisons:
        change = after_val - before_val
        change_str = f"{change:+.1f}%"
        
        # Color coding (for terminals that support it)
        if change > 0:
            change_display = f"âœ… {change_str}"
        elif change < 0:
            change_display = f"âŒ {change_str}"
        else:
            change_display = f"âž– {change_str}"
        
        print(f"{metric_name:<40} {before_val:>6.1f}%      {after_val:>6.1f}%      {change_display}")
    
    print("-" * 80)
    
    # Calculate average improvement
    avg_before = sum(c[1] for c in comparisons) / len(comparisons)
    avg_after = sum(c[2] for c in comparisons) / len(comparisons)
    avg_improvement = avg_after - avg_before
    
    print(f"\n{'Average Improvement:':<40} {avg_improvement:+.1f}%")
    print("=" * 80 + "\n")

def generate_baseline_report():
    """Generate a baseline report for comparison (simulated pre-M4 metrics)"""
    
    # These are estimated pre-M4 metrics based on typical LLM system performance
    baseline = {
        "timestamp": "2024-01-01T00:00:00 (Baseline - Pre-M4)",
        "dataset_summary": get_summary(),
        "total_tests_run": 80,
        "successful_runs": 75,
        "failed_runs": 5,
        "metrics": {
            "classification_accuracy": {
                "metric": "Classification Accuracy",
                "accuracy": 75.0,
                "correct": 60,
                "total": 80
            },
            "answer_grounding": {
                "metric": "Answer Grounding Rate",
                "grounding_rate": 80.0,
                "grounded": 32,
                "total": 40
            },
            "pii_protection": {
                "metric": "PII Protection Rate",
                "protection_rate": 90.0,
                "protected": 9,
                "total": 10
            },
            "professional_tone": {
                "metric": "Professional Tone Compliance",
                "compliance_rate": 85.0,
                "compliant": 68,
                "total": 80
            },
            "error_handling": {
                "metric": "Error Handling Rate",
                "handling_rate": 93.75,
                "handled": 75,
                "total": 80
            },
            "sensitive_escalation": {
                "metric": "Sensitive Content Escalation",
                "escalation_rate": 70.0,
                "escalated": 7,
                "total": 10
            }
        }
    }
    
    return baseline

def get_summary():
    """Placeholder for dataset summary"""
    return {
        "total_cases": 80,
        "faq_cases": 25,
        "data_lookup_cases": 25,
        "escalation_cases": 20,
        "pii_cases": 5,
        "sensitive_cases": 10
    }

def main():
    """Main comparison function"""
    
    if len(sys.argv) < 2:
        print("\n" + "=" * 80)
        print("METRICS COMPARISON TOOL")
        print("=" * 80)
        print("\nUsage:")
        print("  1. Compare with baseline (estimated pre-M4):")
        print("     python tests/compare_metrics.py outputs/metrics_report_XXXXXX.json")
        print("\n  2. Compare two actual reports:")
        print("     python tests/compare_metrics.py before.json after.json")
        print("\nNote: If you provide one file, it will compare against baseline.")
        print("      If you provide two files, it will compare them directly.")
        print("=" * 80 + "\n")
        sys.exit(1)
    
    if len(sys.argv) == 2:
        # Compare with baseline
        after_path = sys.argv[1]
        print(f"\nðŸ“Š Comparing '{after_path}' with baseline (estimated pre-M4 metrics)...")
        
        before_report = generate_baseline_report()
        after_report = load_report(after_path)
        
    else:
        # Compare two reports
        before_path = sys.argv[1]
        after_path = sys.argv[2]
        print(f"\nðŸ“Š Comparing '{before_path}' with '{after_path}'...")
        
        before_report = load_report(before_path)
        after_report = load_report(after_path)
    
    compare_metrics(before_report, after_report)

if __name__ == "__main__":
    main()

